---
title: 'Math 490 HW #11'
author: "Maxwell Levin"
date: "February 28, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment=' ', fig.width = 6, fig.height = 6, fig.align = "center")
```

## Question 1.

#### a. Show the work of finding $\small{\sigma_\psi}$ discussed in Example 1 of this class.

Recall that we are trying to use the Importance Sampling method with Monte Carlo random simulations to get an estimate for 

\[
  I = \int_0^{2\pi}e^{cos(x)}dx.
\]

Also recall that we rewrote this integral as

\[
  I = \int_0^{2\pi} \frac{\psi(x)}{g(x)} dx, 
\]

where $\small{\psi(x)}$ was given by $\small{g(x)e^{cos(x)}}$. We used the central limit theorem to achieve the Monte Carlo Importance Sampling estimator:

\[
  I \approx Z_N^{IS} = \frac{{\Large{\psi}}(Y_1) + {\Large{\psi}}(Y_2) + \dots + {\Large{\psi}}(Y_N)}{N},
\]

where $\small{Y_1, Y_2, \dots, Y_N}$ are independently and identically distributed over the p.d.f. $\small{g(\cdot)}$. Remember that 

\[
g(y) =    \left\{
          \begin{array}{ll}
              \frac{2}{3\pi}, &  \text{if } 0 \leq y < \frac{\pi}{2} ; \\[1mm]
              \frac{1}{3\pi}, &  \text{if } \frac{\pi}{2} \leq y < \frac{3\pi}{2} ; \\[1mm]
              \frac{2}{3\pi}, &  \text{if } \frac{3\pi}{2} \leq y < 2\pi ; \\[1mm]
              0, & \text{otherwise.} \\[1mm]
          \end{array} 
          \right. 
\]

We then have that 

\[
  E[Z_N^{IS}] = E[{\Large{\psi}}(Y_1)] = I,
\]
\[
  \sigma_\psi^2 =  Var[{\Large{\psi}}(Y)] = E[{\Large{\psi}}(Y)^2] - E[{\Large{\psi}}(Y)]^2,
\]
\[
  \sigma_\psi^2 = E\left[\frac{e^{2cos(Y)}}{g(Y)^2}\right] - I^2.
\]

We can compute $\small{E\left[\frac{e^{2cos(Y)}}{g(Y)^2}\right]}$

\[
   = \int_0^{2\pi} ({\Large{\psi}}(Y))^2g(y) dy,
\]
\[
  = \int_0^{2\pi} \frac{1}{g(y)} e^{2cos(y)} dy,
\]
\[
  = \int_0^{\frac{\pi}{2}} \frac{3\pi}{2} e^{2cos(y)} dy 
  + \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} 3\pi e^{2cos(y)} dy 
  + \int_{\frac{3\pi}{2}}^{2\pi} \frac{3\pi}{2} e^{2cos(y)} dy 
  \approx 72.5612
\]


That is, $\small{\sigma^2_\psi}$ is given by

\[
  \sigma^2_\psi \approx 72.5612 - (7.9549)^2 \approx 9.2803.
\]

Thus we have 

\[
  \sigma_\psi \approx 3.0464.
\]



#### b. Show the work of finding the c.d.f. $\small{G(\cdot)}$ and the inverse $\small{G^{-1}(\cdot)}$ discussed in Example 1 of this class.


We find the c.d.f. $\small{G(\cdot)}$ by integrating our p.d.f. $\small{g(\cdot)}$ from $\small{-\infty}$ to $\small{x}$. Although our p.d.f. $\small{g(\cdot)}$ is discontinuous, each component is linear, which makes integration rather trivial. To be explicit, however, we should note the following:

\[
  \int_0^{\frac{\pi}{2}} \frac{2}{3\pi}dt = 
  \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \frac{1}{3\pi}dt =
  \int_{\frac{3\pi}{2}}^{2\pi} \frac{2}{3\pi}dt = \frac{1}{3}.
\]

We can use these results to calculate $\small{G(\cdot)}$:
\[
G(x) =    \left\{
          \begin{array}{ll}
              0,              &  \text{if } x < 0; \\[2mm]
              {\Large\int}_0^{x}\frac{2}{3\pi}dt, &  \text{if } 0 \leq x < \frac{\pi}{2} ; \\[2mm]
              \frac{1}{3} + {\Large\int}_{\frac{\pi}{2}}^{x}\frac{1}{3\pi}dt, &  \text{if } \frac{\pi}{2} \leq x < \frac{3\pi}{2} ; \\[2mm]
             \frac{2}{3} + {\Large\int}_{\frac{3\pi}{2}}^{x}\frac{2}{3\pi}dt, &  \text{if } \frac{3\pi}{2} \leq x < 2\pi ; \\[2mm]
              1,              &  \text{if } x > 2\pi.  \\[2mm]
          \end{array} 
          \right. 
\]

Evaluating the remaining integrals, we see that 

\[
G(x) =    
\left\{
\begin{array}{ll}
  0, &  \text{if } x < 0; \\[2mm]
  \frac{2}{3\pi}x, &  \text{if } 0 \leq x < \frac{\pi}{2} ; \\[2mm]
  \frac{1}{3} + \frac{1}{3\pi}(x - \frac{\pi}{2}), &  \text{if } \frac{\pi}{2} \leq x < \frac{3\pi}{2} ; \\[2mm]
  \frac{2}{3} + \frac{2}{3\pi}(x - \pi), &  \text{if } \frac{3\pi}{2} \leq x < 2\pi ; \\[2mm]
  1, &  \text{if } x > 2\pi.  \\[2mm]
\end{array} 
\right. 
\]

Thus we have found $\small{G(\cdot)}$. We now seek $\small{G^{-1}(\cdot)}$, which we can do by setting $\small{y = G(x)}$ and solving for $\small{y}$ in each section. Doing this we see that $\small{G^{-1}(\cdot)}$ is given by:

\[
G^{-1}(y) =    
\left\{
\begin{array}{ll}
  \frac{3\pi}{2}y, &  \text{if } 0 \leq y < \frac{1}{3} ; \\[2mm]
  \frac{\pi}{2} + 3\pi(y - \frac{1}{3}), &  \text{if } \frac{1}{3} \leq y < \frac{2}{3} ; \\[2mm]
  \pi + \frac{3}{2\pi}(y - \frac{2}{3}), &  \text{if } \frac{2}{3} \leq y \leq 1 ; \\[2mm] 
\end{array} 
\right. 
\]


## Question 2.

#### a. If sample size $\small{N = 100}$, what is the mean and the standard deviation of the importance sampling estimator $\small{Z_{100}^{IS}}$ discussed in Example 1 of this class?

As we know, our mean does not change with our sample size. Thus our mean, $\small{E\left[Z_{100}^{IS}\right]}$, is given by


\[
  E\left[Z_{100}^{IS}\right] = E\left[Z_{1}^{IS}\right] = I \approx 7.9549.
\]

We also know that our standard deviation decreases by a factor of the square root of our sample size. Thus when $\small{N = 100}$ our standard deviation will be a tenth of the standard deviation of $\small{\sigma_\psi}$,

\[
  \sigma_{100} = SD\left[Z_{100}^{IS}\right] = \frac{1}{10}\sigma_\psi \approx 0.30464.
\]

#### b. With sample size $\small{N = 100}$, use R to simulate 1024 importance sampling estimates of $\small{I}$. Make a histogram of those 1024 importance sampling estimates, and report the mean and the standard deviation of those 1024 estimates

```{r, echo=TRUE}
gpdf <- function(x) {  # define p.d.f. g(.) #
	ifelse((x >= 0 & x < (pi/2)), 2/(3*pi),
	  ifelse((x >= (pi/2) & x < (3*pi/2)), 1/(3*pi),
	    ifelse((x >= (3*pi/2) & x <= (2*pi)), 2/(3*pi), 0
	    )
	  )
	)
}

Ginv <- function(y) {  # define c.d.f. inverse G^(-1)(.) #
	ifelse(y < 0, 0,
	  ifelse((y >= 0 & y < 1/3), 3*pi*y/2,
	    ifelse((y >= 1/3 & y < 2/3), 3*pi*y-pi/2,
	      ifelse((y >= 2/3 & y < 1), 3*pi*y/2 + pi/2, 2*pi
	      )
	    )
	  )
	)
}

cdfInv <- function(n) {  # CDF inversion sampling #
	U <- runif(n);
	Ginv(U);
}

clt <- function(sam, rep){
	obs <- NULL;
	for (i in 1:rep){
		y <- cdfInv(sam);
		psi <- exp(cos(y)) / gpdf(y);
		psibar <- mean(psi);
		obs <- c(obs, psibar);
	}
	obs;
}

simN100 = clt(100, 1024)
hist(simN100, breaks=seq(min(simN100), max(simN100)+0.1, 0.1), prob=T)
```

The mean of our simulation is
```{r}
mean(simN100)
```

The standard deviation is
```{r}
sd(simN100)
```

These are not too far off from our calculations.

#### c. If sample size $\small{N = 100}$, how likely is the importance sampling estimator $\small{Z_{100}^{IS}}$ to yield an estimate of $\small{I}$ within error $\small{\pm0.01}$? Use R to simulate this probability with 10000 runs.

We can run the following R code to calculate this probability:

```{r, echo=TRUE}
simN100 = clt(100, 10000)
length(simN100[abs(7.9549 - simN100) <= 0.01]) / length(simN100)
```

Thus we see that we have about a 2% chance of estimating $\small{I}$ within $\small{\pm 0.01}$. This is not great.

#### d. How large should $\small{N}$ be such that the importance sampling estimator $\small{Z_{100}^{IC}}$ would yield an estimate of $\small{I}$ within error $\small{\pm0.01}$ with probability $\small{95\%}$?


Recall from previous homeworks that $\small{N}$ is given by

\[
  N = \left(\frac{1.96\sigma}{\epsilon}\right)^2,
\]
\[
  N \approx \left(\frac{1.96(3.0464)}{0.01}\right)^2 \approx 356,522.
\]




